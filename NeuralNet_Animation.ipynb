{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2  #package for image display\n",
    "import pickle\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, hamming_loss\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#load libraries needed for CNN\n",
    "from vgg16 import VGG16\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras import applications\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.preprocessing import image as image_utils\n",
    "from imagenet_utils import decode_predictions\n",
    "from imagenet_utils import preprocess_input\n",
    "\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Read the Poster Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 103 movies without poster.\n",
      "Final data dimension:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5893, 39)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### read the data\n",
    "data = pd.read_csv(\"data/top1000_movies_2011_2016_tmdb_imdb.csv\")\n",
    "data.shape\n",
    "\n",
    "# extract movies with poster information\n",
    "index = np.zeros(data.shape[0])\n",
    "\n",
    "for i in range(data.shape[0]):\n",
    "    try:\n",
    "        np.isnan(data.iloc[i, 0])\n",
    "    except:\n",
    "        index[i] = 1\n",
    "        \n",
    "print (\"There are {} movies without poster.\".format(np.sum(index == 0)))\n",
    "\n",
    "# extract data with poster\n",
    "data_with_poster = data.iloc[index == 1, ]\n",
    "print (\"Final data dimension:\")\n",
    "data_with_poster.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Final data shape:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5893, 180, 128, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### read each image and resize ######\n",
    "# Each poster is read and normalized\n",
    "# inserted to the center of an array of zeros to ensure the same size\n",
    "\n",
    "# total number of posters\n",
    "n = data_with_poster.shape[0]\n",
    "\n",
    "# target image size\n",
    "target_length = 180\n",
    "target_width = 128\n",
    "\n",
    "# np.array to store the whole matrix in the order of the movie id\n",
    "img_matrix = np.zeros([n, target_length, target_width, 3])\n",
    "\n",
    "for i in range(n):\n",
    "    if i % 1000 == 0:    \n",
    "        print (i)    \n",
    "    \n",
    "    # read each movie id\n",
    "    movie_id = data_with_poster[\"id\"].values[i]\n",
    "    # construct the poster path\n",
    "    path = 'data/posters/' + str(movie_id) + '.jpg'\n",
    "    \n",
    "    # read image\n",
    "    img = cv2.imread(path)\n",
    "    \n",
    "    # normalize the image[-1,1], center at 0\n",
    "    # so that the zero padding does not affect the image\n",
    "    img = img.astype('float32')\n",
    "    img = (img - 255/2)/255\n",
    "    \n",
    "    # extract the length and width of the image\n",
    "    img_length, img_width = np.asarray(img.shape[:2])\n",
    "    \n",
    "    ## put the image to the center of the array\n",
    "    # calcuate location of the left edge\n",
    "    left_loc = int(0.5*(target_width - img_width))\n",
    "    # calcuate location of the top edge\n",
    "    top_loc = int(0.5*(target_length - img_length))\n",
    "    \n",
    "    # store the image\n",
    "    img_matrix[i, top_loc:(top_loc+img_length), \n",
    "               left_loc:(left_loc+img_width), :] = img\n",
    "    \n",
    "print (\"Final data shape:\")\n",
    "img_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape: (5893, 180, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "# reformat the data shape\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    img_matrix = img_matrix.reshape(n, 3, target_length, target_width)\n",
    "    input_shape = (3, target_length, target_width)\n",
    "else:\n",
    "    img_matrix = img_matrix.reshape(n, target_length, target_width, 3)\n",
    "    input_shape = (target_length, target_width, 3)\n",
    "\n",
    "print('data shape:', img_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### extract and clean the response variable \n",
    "# convert genre ids to list\n",
    "y = list()\n",
    "for i in range(data_with_poster.shape[0]):\n",
    "    genre = map(int, re.sub(\"[\\[ \\] ]\", \"\", data['genre_ids'][i]).split(','))\n",
    "    y.append(genre)\n",
    "\n",
    "# binarize response variable (returns array)\n",
    "y_binary = MultiLabelBinarizer().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### split the train and test data (9:1 ratio)\n",
    "# shuffle the samples\n",
    "seed = 109\n",
    "index_2 = range(n)\n",
    "shuffle(index_2)\n",
    "\n",
    "img_matrix = img_matrix[index_2]\n",
    "y_binary = y_binary[index_2]\n",
    "\n",
    "# use 90% of the data as training data to fit the model\n",
    "split_ratio = 0.9\n",
    "split_num = int(n * split_ratio)\n",
    "\n",
    "x_train = img_matrix[:split_num]\n",
    "x_test = img_matrix[split_num:]\n",
    "\n",
    "y_train = y_binary[:split_num]\n",
    "y_test = y_binary[split_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.047457627118644069"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the percentage of animation movies in the test data\n",
    "np.mean(y_test[:, 2] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subsample the training data for more balanced classes (from 19:1 to 2:1)\n",
    "# the size of non-animation movies\n",
    "size = 2 * np.sum(y_train[:, 2] == 1)\n",
    "\n",
    "# extract movies with animation label\n",
    "x_animation = x_train[y_train[:, 2] == 1, ]\n",
    "y_animation = y_train[y_train[:, 2] == 1, ]\n",
    "\n",
    "# sample from the rest of data\n",
    "# non-animation movies\n",
    "x_no_animation = x_train[y_train[:, 2] != 1, ]\n",
    "y_no_animation = y_train[y_train[:, 2] != 1, ]\n",
    "# suffle the data\n",
    "index = range(x_no_animation.shape[0])\n",
    "shuffle(index)\n",
    "# extract the needed number of movies from the top of the shuffled list\n",
    "x_no_subsample = x_no_animation[index[1:size], ]\n",
    "y_no_subsample = y_no_animation[index[1:size], ]\n",
    "\n",
    "# mix the animation and non-animation movies and shuffle\n",
    "x_subsample = np.concatenate([x_animation, x_no_subsample], axis = 0)\n",
    "y_subsample = np.concatenate([y_animation, y_no_subsample], axis = 0)\n",
    "\n",
    "index = range(x_subsample.shape[0])\n",
    "shuffle(index)\n",
    "\n",
    "x_subsample = x_subsample[index]\n",
    "y_subsample = y_subsample[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Import VGG16 and overfit on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### load pretrained model - VGG16\n",
    "base_model =VGG16(weights='imagenet', include_top=False)\n",
    "x = base_model.output \n",
    "\n",
    "# add fully-connected layer\n",
    "x = GlobalAveragePooling2D()(x) \n",
    "# add a dense layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# the last layer has 1 class because we are predicting only 1 genre\n",
    "# the activation should be sigmoid for binary prediction\n",
    "predictions = Dense(1, activation='sigmoid')(x) \n",
    "\n",
    "# the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# set the first 11 layers (up to the second last conv block)\n",
    "# to non-trainable (weights will not be updated)\n",
    "# only train the early convolutionary layers\n",
    "for layer in model.layers[:12]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.001, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "### specify the tuning parameters\n",
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 20\n",
    "# number of iterations over the complete training data (chosen after trials)\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the model\n",
    "history = model.fit(x_train, y_train[:, 2],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# save the intermediate result\n",
    "model.save('finetune_base_animation.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fine tune the final layers using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_model = load_model('finetune_base_animation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# add dropouts and new dense layers\n",
    "x = base_model.layers[19].output \n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "predictions = Dense(1, activation='sigmoid')(x) # sigmoid for binary prediction\n",
    "model2 = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set the first 19 layers (all the convolutionary blocks)\n",
    "# to non-trainable (weights will not be updated)\n",
    "for layer in model2.layers[:20]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "batch_size = 20\n",
    "epochs = 40\n",
    "## can add early stopping (not here to better show the result)\n",
    "#earlyStopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# compile the model\n",
    "model2.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.SGD(lr=0.0001, momentum=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train the model on 90% of the training data\n",
    "# the rest 10% is validation data \n",
    "history = model2.fit(x_train, y_train[:, 2],\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1, \n",
    "                    #callbacks=[earlyStopping],\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# predict on test data\n",
    "y_pred = model2.predict(x_test)\n",
    "y_pred = y_pred.reshape(len(y_pred))\n",
    "# convert the result to binary using 0.5 as threshold\n",
    "y_bin = np.zeros(len(y_pred))\n",
    "y_bin[y_pred > 0.5] = 1\n",
    "\n",
    "# extract the actual genre for each movie\n",
    "y_actual = y_test[:, 2].reshape(len(y_pred))\n",
    "\n",
    "# save the data into a csv\n",
    "test_result = pd.DataFrame({\n",
    "    \"prediction\": y_bin,\n",
    "    \"actual\": y_actual,\n",
    "    \"probability\": y_pred\n",
    "})\n",
    "\n",
    "test_result.to_csv(\"prediction_animation.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.114285714286\n",
      "Accuracy:  0.684745762712\n",
      "Precision:  0.0722891566265\n",
      "Recall:  0.272727272727\n",
      "Hamming loss: 0.315254237288\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[392, 154],\n",
       "       [ 32,  12]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the above code was run on AWS, and test prediction was saved\n",
    "# read the prediction and compute evaluation metrics\n",
    "result = pd.read_csv(\"prediction_animation.csv\")\n",
    "\n",
    "print \"F1 score: \", f1_score(result[\"actual\"], result[\"prediction\"])\n",
    "print \"Accuracy: \", np.mean(result[\"actual\"] == result[\"prediction\"])\n",
    "print \"Precision: \", precision_score(result[\"actual\"], result[\"prediction\"])\n",
    "print \"Recall: \", recall_score(result[\"actual\"], result[\"prediction\"])\n",
    "print \"Hamming loss:\", hamming_loss(result[\"actual\"], result[\"prediction\"])\n",
    "confusion_matrix(result[\"actual\"], result[\"prediction\"])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
